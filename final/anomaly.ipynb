{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a1cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0 — Imports & Device\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import glob\n",
    "import torch\n",
    "import torchaudio\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial import distance\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "alpha = 0.5\n",
    "m = 0.7\n",
    "gpu_num = 0\n",
    "epoch = 300\n",
    "num_classes = 41\n",
    "split_ratio = 0.95\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "mode = \"noisy_arcmix\"\n",
    "\n",
    "save_path = r\"D:\\DCASE2020\\Noisy-ArcMix\\check_points\\TASTgram_noisy_arcmix.pth\"\n",
    "root_path = r\"D:\\DCASE2020\\Noisy-ArcMix\\datasets\"\n",
    "name_list = ['fan', 'pump', 'slider', 'ToyCar', 'ToyConveyor', 'valve']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf20764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Class Dataset\n",
    "# ============================\n",
    "\n",
    "def file_to_log_mel_spectrogram(y, sr, n_mels, n_fft, hop_length, power):\n",
    "    transform = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power, pad_mode='constant', norm='slaney', mel_scale='slaney')\n",
    "    mel_spectrogram = transform(y)\n",
    "    log_mel_spectrogram = 20.0 / power * torch.log10(mel_spectrogram + sys.float_info.epsilon)\n",
    "    return log_mel_spectrogram\n",
    "\n",
    "class test_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_path, test_name, name_list):\n",
    "        dataset_dir = os.path.join(root_path, test_name, 'test')\n",
    "        normal_files = sorted(glob.glob('{dir}/normal_*'.format(dir=dataset_dir)))\n",
    "        anomaly_files = sorted(glob.glob('{dir}/anomaly_*'.format(dir=dataset_dir)))\n",
    "        \n",
    "        self.test_files = np.concatenate((normal_files, anomaly_files), axis=0)\n",
    "        \n",
    "        normal_labels = np.zeros(len(normal_files))\n",
    "        anomaly_labels = np.ones(len(anomaly_files))\n",
    "        self.y_true = torch.LongTensor(np.concatenate((normal_labels, anomaly_labels), axis=0))\n",
    "        \n",
    "        target_idx = name_list.index(test_name)\n",
    "        \n",
    "        label_init_num = 0\n",
    "        for i, name in enumerate(name_list):\n",
    "            if i == target_idx:\n",
    "                break\n",
    "            label_init_num+=len(self._get_label_list(name))\n",
    "            \n",
    "        self.labels = []\n",
    "        label_list = self._get_label_list(test_name)\n",
    "        for file_name in self.test_files:\n",
    "            for idx, label_idx in enumerate(label_list):\n",
    "                if label_idx in file_name:\n",
    "                    self.labels.append(idx + label_init_num)\n",
    "        \n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "        \n",
    "        self.y_list = []\n",
    "        self.y_spec_list = []\n",
    "        \n",
    "        for i in tqdm(range(len(self.test_files))):\n",
    "            y, sr = self._file_load(self.test_files[i])\n",
    "            y_specgram = file_to_log_mel_spectrogram(y, sr, n_fft=2048, hop_length=512, n_mels=128, power=2)\n",
    "            self.y_list.append(y)\n",
    "            self.y_spec_list.append(y_specgram)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anomal_label = self.y_true[idx]\n",
    "        label = self.labels[idx]\n",
    "        return self.y_list[idx], self.y_spec_list[idx], label, anomal_label \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_files)\n",
    "    \n",
    "    def _file_load(self, file_name):\n",
    "        try:\n",
    "            y, sr = torchaudio.load(file_name)\n",
    "            y = y[..., :sr * 10]\n",
    "            return y, sr\n",
    "        except:\n",
    "            print(\"file_broken or not exists!! : {}\".format(file_name))\n",
    "    \n",
    "    def _get_label_list(self, name):\n",
    "        if name == 'ToyConveyor':\n",
    "            label_list = ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06'] \n",
    "    \n",
    "        elif name == 'ToyCar':\n",
    "            label_list = ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07']\n",
    "        \n",
    "        else:\n",
    "            label_list = ['id_00', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06']\n",
    "            \n",
    "        return label_list\n",
    "\n",
    "class train_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_path, name_list): \n",
    "        data_path = [os.path.join(root_path, name, 'train') for name in name_list]\n",
    "        \n",
    "        files_list= [self._file_list_generator(target_path) for target_path in data_path]\n",
    "        \n",
    "        self.labels = []\n",
    "\n",
    "        maximum = 0\n",
    "        for i, files in enumerate(files_list):\n",
    "            label_list = self._get_label_list(name_list[i])\n",
    "            for file_name in files:\n",
    "                for idx, label_idx in enumerate(label_list):\n",
    "                    if label_idx in file_name:\n",
    "                        self.labels.append(idx + maximum)\n",
    "            maximum = max(self.labels)+1\n",
    "\n",
    "        self.unrolled_files_list = list(itertools.chain.from_iterable(files_list))\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "        \n",
    "        self.y_list = []\n",
    "        self.y_spec_list = []\n",
    "        \n",
    "        for i in tqdm(range(len(self.unrolled_files_list))):\n",
    "            y, sr = self._file_load(self.unrolled_files_list[i])\n",
    "            y_specgram = file_to_log_mel_spectrogram(y, sr, n_fft=2048, hop_length=512, n_mels=128, power=2)\n",
    "            self.y_list.append(y)\n",
    "            self.y_spec_list.append(y_specgram)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.y_list[idx], self.y_spec_list[idx], self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.unrolled_files_list)\n",
    "      \n",
    "    def _get_label_list(self, name):\n",
    "        if name == 'ToyConveyor':\n",
    "            label_list = ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06'] \n",
    "    \n",
    "        elif name == 'ToyCar':\n",
    "            label_list = ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07']\n",
    "        \n",
    "        else:\n",
    "            label_list = ['id_00', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06']\n",
    "            \n",
    "        return label_list \n",
    "    \n",
    "    def _file_list_generator(self, target_dir):\n",
    "        training_list_path = os.path.abspath('{dir}/*.wav'.format(dir=target_dir))\n",
    "        files = sorted(glob.glob(training_list_path))\n",
    "        if len(files) == 0:\n",
    "            print('no_wav_file!!')\n",
    "        return files\n",
    "    \n",
    "    def _file_load(self, file_name):\n",
    "        try:\n",
    "            y, sr = torchaudio.load(file_name)\n",
    "            y = y[..., :sr * 10]\n",
    "            return y, sr\n",
    "        except:\n",
    "            print(\"file_broken or not exists!! : {}\".format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839f6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Mode\n",
    "# ============================\n",
    "\n",
    "def get_accuracy(logits, y):\n",
    "    pred_label = torch.argmax(logits, dim=-1)\n",
    "    return torch.sum(pred_label == y)/len(pred_label)\n",
    "  \n",
    "def dataset_split(dataset, split_ratio):\n",
    "    data_len = len(dataset)\n",
    "    \n",
    "    train_len = int(data_len * split_ratio)\n",
    "    valid_len = data_len - train_len\n",
    "    \n",
    "    train_dataset, valid_dataset = random_split(dataset, [train_len, valid_len])\n",
    "\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "def mixup_data(x_wavs, x_mels, y, device, alpha=0.5):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x_mels.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    \n",
    "    mixed_x_wavs = lam * x_wavs + (1 - lam) * x_wavs[index, :]\n",
    "    mixed_x_mels = lam * x_mels + (1 - lam) * x_mels[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x_wavs, mixed_x_mels, y_a, y_b, lam\n",
    "\n",
    "def noisy_arcmix_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def arcmix_criterion(criterion, pred, pred2, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred2, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ab52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Loss & Network\n",
    "# ============================\n",
    "\n",
    "class ASDLoss(nn.Module):\n",
    "    def __init__(self, reduction=True):\n",
    "        super(ASDLoss, self).__init__()\n",
    "        if reduction == True:\n",
    "            self.ce = nn.CrossEntropyLoss()\n",
    "        \n",
    "        else:\n",
    "            self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        loss = self.ce(logits, labels)\n",
    "        return loss\n",
    "\n",
    "# ArcFace is referred to https://github.com/ronghuaiyang/arcface-pytorch/blob/master/models/metrics.py\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features=128, out_features=200, s=30.0, m=0.7, sub=1, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.sub = sub\n",
    "        self.weight = Parameter(torch.Tensor(out_features * sub, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        \n",
    "        # make the function cos(theta+m) monotonic decreasing while theta in [0°,180°]\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        cosine = F.linear(F.normalize(x), F.normalize(self.weight))\n",
    "        \n",
    "        if self.sub > 1:\n",
    "            cosine = cosine.view(-1, self.out_features, self.sub)\n",
    "            cosine, _ = torch.max(cosine, dim=2)\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        \n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where((cosine - self.th) > 0, phi, cosine - self.mm)\n",
    "\n",
    "        one_hot = torch.zeros(cosine.size(), device=x.device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output = output * self.s\n",
    "        return output\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expansion):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.connect = stride == 1 and inp == oup\n",
    "        #\n",
    "        self.conv = nn.Sequential(\n",
    "            # pw\n",
    "            nn.Conv2d(inp, inp * expansion, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(inp * expansion),\n",
    "            nn.PReLU(inp * expansion),\n",
    "            # dw\n",
    "            nn.Conv2d(inp * expansion, inp * expansion, 3, stride, 1, groups=inp * expansion, bias=False),\n",
    "            nn.BatchNorm2d(inp * expansion),\n",
    "            nn.PReLU(inp * expansion),\n",
    "\n",
    "            # pw-linear\n",
    "            nn.Conv2d(inp * expansion, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inp, oup, k, s, p, dw=False, linear=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.linear = linear\n",
    "        if dw:\n",
    "            self.conv = nn.Conv2d(inp, oup, k, s, p, groups=inp, bias=False)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(inp, oup, k, s, p, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(oup)\n",
    "        if not linear:\n",
    "            self.prelu = nn.PReLU(oup)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.linear:\n",
    "            return x\n",
    "        else:\n",
    "            return self.prelu(x)\n",
    "\n",
    "\n",
    "#https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Liu_8_t2.pdf\n",
    "Mobilefacenet_bottleneck_setting = [\n",
    "    # t, c , n ,s\n",
    "    [2, 128, 2, 2],\n",
    "    [4, 128, 2, 2],\n",
    "    [4, 128, 2, 2],\n",
    "]\n",
    "\n",
    "\n",
    "class MobileFaceNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_class,\n",
    "                 bottleneck_setting=Mobilefacenet_bottleneck_setting):\n",
    "        super(MobileFaceNet, self).__init__()\n",
    "\n",
    "        self.conv1 = ConvBlock(3, 64, 3, 2, 1)\n",
    "\n",
    "        self.dw_conv1 = ConvBlock(64, 64, 3, 1, 1, dw=True)\n",
    "\n",
    "        self.inplanes = 64\n",
    "        block = Bottleneck\n",
    "        self.blocks = self._make_layer(block, bottleneck_setting)\n",
    "\n",
    "        self.conv2 = ConvBlock(bottleneck_setting[-1][1], 512, 1, 1, 0)\n",
    "        # 20(10), 4(2), 8(4)\n",
    "        self.linear7 = ConvBlock(512, 512, (8, 20), 1, 0, dw=True, linear=True)\n",
    "        \n",
    "        self.linear1 = ConvBlock(512, 128, 1, 1, 0, linear=True)\n",
    "\n",
    "        self.fc_out = nn.Linear(128, num_class)\n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, setting):\n",
    "        layers = []\n",
    "        for t, c, n, s in setting:\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    layers.append(block(self.inplanes, c, s, t))\n",
    "                else:\n",
    "                    layers.append(block(self.inplanes, c, 1, t))\n",
    "                self.inplanes = c\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.dw_conv1(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.linear7(x)\n",
    "        x = self.linear1(x)\n",
    "        feature = x.view(x.size(0), -1)\n",
    "        out = self.fc_out(feature)\n",
    "        return out, feature\n",
    "\n",
    "\n",
    "class TgramNet(nn.Module):\n",
    "    def __init__(self, num_layer=3, mel_bins=128, win_len=1024, hop_len=512):\n",
    "        super(TgramNet, self).__init__()\n",
    "        # if \"center=True\" of stft, padding = win_len / 2\n",
    "        self.conv_extrctor = nn.Conv1d(1, mel_bins, win_len, hop_len, win_len // 2, bias=False)\n",
    "        self.conv_encoder = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                # 313(10) , 63(2), 126(4)\n",
    "                nn.LayerNorm(313),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv1d(mel_bins, mel_bins, 3, 1, 1, bias=False),\n",
    "            ) for _ in range(num_layer)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_extrctor(x)\n",
    "        out = self.conv_encoder(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TASTgramMFN(nn.Module):\n",
    "    def __init__(self, num_classes, mode,\n",
    "                c_dim=128,\n",
    "                win_len=1024,\n",
    "                hop_len=512,\n",
    "                bottleneck_setting=Mobilefacenet_bottleneck_setting,\n",
    "                use_arcface=True, m=0.7, s=30, sub=1\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.arcface = ArcMarginProduct(in_features=c_dim, out_features=num_classes,\n",
    "                                        m=m, s=s, sub=sub) if use_arcface else use_arcface\n",
    "        self.tgramnet = TgramNet(mel_bins=c_dim, win_len=win_len, hop_len=hop_len)\n",
    "        self.mobilefacenet = MobileFaceNet(num_class=num_classes,\n",
    "                                          bottleneck_setting=bottleneck_setting)\n",
    "        self.mode = mode\n",
    "        \n",
    "        if mode not in ['arcface', 'arcmix', 'noisy_arcmix']:\n",
    "            raise ValueError('Choose one of [arcface, arcmix, noisy_arcmix]')\n",
    "        \n",
    "        self.temporal_attention = Temporal_Attention(feature_dim=c_dim)\n",
    "        \n",
    "    def get_tgram(self, x_wav):\n",
    "        return self.tgramnet(x_wav)\n",
    "\n",
    "    def forward(self, x_wav, x_mel, label, train=True):\n",
    "        x_t = self.tgramnet(x_wav).unsqueeze(1)\n",
    "        \n",
    "        x_mel_temp_att = self.temporal_attention(x_mel).unsqueeze(1)\n",
    "        \n",
    "        x = torch.cat((x_t, x_mel, x_mel_temp_att), dim=1)\n",
    "        \n",
    "        out, feature = self.mobilefacenet(x)\n",
    "        \n",
    "        if self.mode == 'arcmix':\n",
    "            if train:\n",
    "                out = self.arcface(feature, label[0])\n",
    "                out_shuffled = self.arcface(feature, label[1])\n",
    "                return out, out_shuffled, feature\n",
    "            else:\n",
    "                out = self.arcface(feature, label)\n",
    "                return out, feature\n",
    "        \n",
    "        else:\n",
    "            out = self.arcface(feature, label)\n",
    "            return out, feature\n",
    "        \n",
    "        \n",
    "class Temporal_Attention(nn.Module):\n",
    "  def __init__(self, feature_dim=128):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.feature_dim = feature_dim\n",
    "    self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "    self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # x: (B, 1, 128, 313)\n",
    "    x = x.squeeze(1)\n",
    "    \n",
    "    x = x.transpose(1,2) # (B, 313, 128)\n",
    "\n",
    "    x1 = self.max_pool(x) # (B, 313, 1)\n",
    "    x2 = self.avg_pool(x) # (B, 313, 1)\n",
    "    \n",
    "    feats = x1 + x2\n",
    "    \n",
    "    feats = feats.repeat(1, 1, self.feature_dim)\n",
    "    \n",
    "    refined_feats = self.sigmoid(feats).transpose(1,2) * x.transpose(1,2)\n",
    "    return refined_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daffcaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "\n",
    "net = TASTgramMFN(num_classes=41, m=0.7, mode=mode).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b85007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Trainer\n",
    "# ============================\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, device, mode, m, alpha, epochs=300, class_num=41, lr=1e-4):\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.net = TASTgramMFN(num_classes=class_num, mode=mode, use_arcface=True, m=m).to(self.device)\n",
    "        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=lr)\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=epochs, eta_min=0.1*float(lr))\n",
    "        self.criterion = ASDLoss().to(self.device)\n",
    "        self.test_criterion = ASDLoss(reduction=False).to(self.device)\n",
    "        self.mode = mode\n",
    "        \n",
    "        if mode not in ['arcface', 'arcmix', 'noisy_arcmix']:\n",
    "            raise ValueError('Mode should be one of [arcface, arcmix, noisy_arcmix]')\n",
    "        \n",
    "        print(f'{mode} mode has been selected...')\n",
    "        \n",
    "    def train(self, train_loader, valid_loader, save_path):\n",
    "        num_steps = len(train_loader)\n",
    "        min_val_loss = 1e10\n",
    "        \n",
    "        for epoch in tqdm(range(self.epochs), total=self.epochs):\n",
    "            sum_loss = 0.\n",
    "            sum_accuracy = 0.\n",
    "            \n",
    "            for _, (x_wavs, x_mels, labels) in tqdm(enumerate(train_loader), total=num_steps):\n",
    "                self.net.train()\n",
    "                \n",
    "                x_wavs, x_mels, labels = x_wavs.to(self.device), x_mels.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                with autocast():\n",
    "                    if self.mode == 'arcface':\n",
    "                        logits, _ = self.net(x_wavs, x_mels, labels)\n",
    "                        loss = self.criterion(logits, labels)\n",
    "                    \n",
    "                    elif self.mode == 'noisy_arcmix':\n",
    "                        mixed_x_wavs, mixed_x_mels, y_a, y_b, lam = mixup_data(x_wavs, x_mels, labels, self.device, alpha=self.alpha)\n",
    "                        logits, _ = self.net(mixed_x_wavs, mixed_x_mels, labels)\n",
    "                        loss = noisy_arcmix_criterion(self.criterion, logits, y_a, y_b, lam)\n",
    "                    \n",
    "                    elif self.mode == 'arcmix':\n",
    "                        mixed_x_wavs, mixed_x_mels, y_a, y_b, lam = mixup_data(x_wavs, x_mels, labels, self.device, alpha=self.alpha)\n",
    "                        logits, logits_shuffled, _ = self.net(mixed_x_wavs, mixed_x_mels, [y_a, y_b])\n",
    "                        loss = arcmix_criterion(self.criterion, logits, logits_shuffled, y_a, y_b, lam)\n",
    "                \n",
    "                sum_accuracy += get_accuracy(logits, labels)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                sum_loss += loss.item()\n",
    "            self.scheduler.step()\n",
    "                \n",
    "            avg_loss = sum_loss / num_steps\n",
    "            avg_accuracy = sum_accuracy / num_steps\n",
    "            \n",
    "            valid_loss, valid_accuracy = self.valid(valid_loader)\n",
    "            \n",
    "            if min_val_loss > valid_loss:\n",
    "                min_val_loss = valid_loss\n",
    "                lr = self.scheduler.get_last_lr()[0]\n",
    "                print(\"model has been saved!\")\n",
    "                print(f'lr: {lr:.7f} | EPOCH: {epoch} | Train_loss: {avg_loss:.5f} | Train_accuracy: {avg_accuracy:.5f} | Valid_loss: {valid_loss:.5f} | Valid_accuracy: {valid_accuracy:.5f}')\n",
    "                # torch.save(self.net.state_dict(), save_path)\n",
    "                \n",
    "    def valid(self, valid_loader):\n",
    "        self.net.eval()\n",
    "        \n",
    "        num_steps = len(valid_loader)\n",
    "        sum_loss = 0.\n",
    "        sum_accuracy = 0.\n",
    "        \n",
    "        for (x_wavs, x_mels, labels) in valid_loader:\n",
    "            x_wavs, x_mels, labels = x_wavs.to(self.device), x_mels.to(self.device), labels.to(self.device)\n",
    "            logits, _ = self.net(x_wavs, x_mels, labels, train=False)\n",
    "            sum_accuracy += get_accuracy(logits, labels)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            sum_loss += loss.item()\n",
    "            \n",
    "        avg_loss = sum_loss / num_steps \n",
    "        avg_accuracy = sum_accuracy / num_steps \n",
    "        return avg_loss, avg_accuracy\n",
    "    \n",
    "    def test(self, test_loader):\n",
    "        self.net.eval()\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        sum_accuracy = 0.\n",
    "        with torch.no_grad():\n",
    "            for x_wavs, x_mels, labels, AN_N_labels in test_loader:\n",
    "                x_wavs, x_mels, labels, AN_N_labels = x_wavs.to(self.device), x_mels.to(self.device), labels.to(self.device), AN_N_labels.to(self.device)\n",
    "                \n",
    "                logits, _ = self.net(x_wavs, x_mels, labels, train=False)\n",
    "                score = self.test_criterion(logits, labels)\n",
    "                sum_accuracy += get_accuracy(logits, labels)\n",
    "                \n",
    "                y_pred.extend(score.tolist())\n",
    "                y_true.extend(AN_N_labels.tolist())\n",
    "        auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "        #pauc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
    "        return auc, sum_accuracy / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eaf5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Evaluator\n",
    "# ============================\n",
    "\n",
    "def evaluator(net, test_loader, criterion, device):\n",
    "    net.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_wavs, x_mels, labels, AN_N_labels in test_loader:\n",
    "            x_wavs, x_mels, labels, AN_N_labels = x_wavs.to(device), x_mels.to(device), labels.to(device), AN_N_labels.to(device)\n",
    "            \n",
    "            logits, _ = net(x_wavs, x_mels, labels, train=False)\n",
    "            \n",
    "            score = criterion(logits, labels)\n",
    "\n",
    "            y_pred.extend(score.tolist())\n",
    "            y_true.extend(AN_N_labels.tolist())\n",
    "    \n",
    "    auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "    pauc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
    "    return auc, pauc                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6b82d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20119/20119 [04:04<00:00, 82.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Train Dataset\n",
    "# ============================\n",
    "\n",
    "print('training dataset loading...')\n",
    "train_ds = train_dataset(root_path, name_list)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eadf7f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Anomaly Score Function\n",
    "# ============================\n",
    "\n",
    "def anomaly_scores(net, arcface, train_loader, test_loader, device):\n",
    "    net.eval()\n",
    "\n",
    "    # === Extract train features (normalized) ===\n",
    "    train_features = []\n",
    "    with torch.no_grad():\n",
    "        for x_wavs, x_mels, labels in train_loader:\n",
    "            x_wavs, x_mels, labels = x_wavs.to(device), x_mels.to(device), labels.to(device)\n",
    "            _, features = net(x_wavs, x_mels, labels, train=False)\n",
    "            train_features.append(features.cpu())\n",
    "    train_features = torch.cat(train_features, dim=0).to(device)\n",
    "    train_norm = F.normalize(train_features, dim=1)\n",
    "    train_np = train_norm.cpu().numpy()\n",
    "\n",
    "    # === Extract test features and ground truth labels ===\n",
    "    test_features = []\n",
    "    y_true = []\n",
    "    logits_list = []\n",
    "    label_list = []\n",
    "    with torch.no_grad():\n",
    "        for x_wavs, x_mels, labels, AN_N_labels in test_loader:\n",
    "            x_wavs, x_mels, labels, AN_N_labels = x_wavs.to(device), x_mels.to(device), labels.to(device), AN_N_labels.to(device)\n",
    "            logits, features = net(x_wavs, x_mels, labels, train=False)\n",
    "            test_features.append(features.cpu())\n",
    "            logits_list.append(logits.cpu())\n",
    "            label_list.append(labels.cpu())\n",
    "            y_true.extend(AN_N_labels.cpu().numpy())\n",
    "\n",
    "    test_features = torch.cat(test_features, dim=0).to(device)\n",
    "    test_norm = F.normalize(test_features, dim=1)\n",
    "    test_np = test_norm.cpu().numpy()\n",
    "    y_true = np.array(y_true)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # === 1. Cross Entropy\n",
    "    logits_all = torch.cat(logits_list, dim=0)\n",
    "    labels_all = torch.cat(label_list, dim=0)\n",
    "    ce_criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    ce_losses = ce_criterion(logits_all, labels_all)\n",
    "    ce_scores = ce_losses.detach().cpu().numpy()\n",
    "\n",
    "    results['CrossEntropy'] = (\n",
    "        roc_auc_score(y_true, ce_scores),\n",
    "        roc_auc_score(y_true, ce_scores, max_fpr=0.1)\n",
    "    )\n",
    "\n",
    "    # # === 2. Cosine Similarity using ArcFace Centers\n",
    "    # arc_weights = arcface.weight.to(device)\n",
    "    # arc_centers = arc_weights.view(arcface.out_features, arcface.sub, -1).mean(dim=1)\n",
    "    # arc_centers = F.normalize(arc_centers, dim=1)\n",
    "\n",
    "    # cos_sim = F.cosine_similarity(test_norm.unsqueeze(1), arc_centers.unsqueeze(0), dim=2)\n",
    "    # best_cos = torch.abs(cos_sim).max(dim=1)[0]\n",
    "    # cos_scores = -torch.log(best_cos + 1e-8).detach().cpu().numpy()\n",
    "\n",
    "    # results['Cosine'] = (\n",
    "    #     roc_auc_score(y_true, cos_scores),\n",
    "    #     roc_auc_score(y_true, cos_scores, max_fpr=0.1)\n",
    "    # )\n",
    "\n",
    "    # === 3. KMeans=60\n",
    "    km = KMeans(n_clusters=60, random_state=0).fit(train_np)\n",
    "    km_centers = torch.tensor(km.cluster_centers_, dtype=torch.float32).to(device)\n",
    "    km_centers = F.normalize(km_centers, dim=1)\n",
    "\n",
    "    cos_sim_km = F.cosine_similarity(test_norm.unsqueeze(1), km_centers.unsqueeze(0), dim=2)\n",
    "    best_km = torch.abs(cos_sim_km).max(dim=1)[0]\n",
    "    kmeans_scores = -torch.log(best_km + 1e-8).cpu().numpy()\n",
    "\n",
    "    results['KMeans=60'] = (\n",
    "        roc_auc_score(y_true, kmeans_scores),\n",
    "        roc_auc_score(y_true, kmeans_scores, max_fpr=0.1)\n",
    "    )\n",
    "\n",
    "    # === 3. KMeans=80\n",
    "    km = KMeans(n_clusters=60, random_state=0).fit(train_np)\n",
    "    km_centers = torch.tensor(km.cluster_centers_, dtype=torch.float32).to(device)\n",
    "    km_centers = F.normalize(km_centers, dim=1)\n",
    "\n",
    "    cos_sim_km = F.cosine_similarity(test_norm.unsqueeze(1), km_centers.unsqueeze(0), dim=2)\n",
    "    best_km = torch.abs(cos_sim_km).max(dim=1)[0]\n",
    "    kmeans_scores = -torch.log(best_km + 1e-8).cpu().numpy()\n",
    "\n",
    "    results['KMeans=80'] = (\n",
    "        roc_auc_score(y_true, kmeans_scores),\n",
    "        roc_auc_score(y_true, kmeans_scores, max_fpr=0.1)\n",
    "    )\n",
    "\n",
    "    # === 4. GMM=60\n",
    "    gmm = GaussianMixture(n_components=40, random_state=0, covariance_type='full').fit(train_np)\n",
    "    gmm_scores = -gmm.score_samples(test_np)\n",
    "    results['GMM=40'] = (\n",
    "        roc_auc_score(y_true, gmm_scores),\n",
    "        roc_auc_score(y_true, gmm_scores, max_fpr=0.1)\n",
    "    )\n",
    "\n",
    "    # === 4. GMM=80\n",
    "    gmm = GaussianMixture(n_components=60, random_state=0, covariance_type='full').fit(train_np)\n",
    "    gmm_scores = -gmm.score_samples(test_np)\n",
    "    results['GMM=60'] = (\n",
    "        roc_auc_score(y_true, gmm_scores),\n",
    "        roc_auc_score(y_true, gmm_scores, max_fpr=0.1)\n",
    "    )\n",
    "\n",
    "    # === 5. LOF=5\n",
    "    lof = LocalOutlierFactor(n_neighbors=5, novelty=True).fit(train_np)\n",
    "    lof_scores = -lof.score_samples(test_np)\n",
    "    results['LOF=5'] = (\n",
    "        roc_auc_score(y_true, lof_scores),\n",
    "        roc_auc_score(y_true, lof_scores, max_fpr=0.1)\n",
    "    )\n",
    "\n",
    "    # === 5. LOF=10\n",
    "    lof = LocalOutlierFactor(n_neighbors=10, novelty=True).fit(train_np)\n",
    "    lof_scores = -lof.score_samples(test_np)\n",
    "    results['LOF=10'] = (\n",
    "        roc_auc_score(y_true, lof_scores),\n",
    "        roc_auc_score(y_true, lof_scores, max_fpr=0.1)\n",
    "    )\n",
    "\n",
    "    # === 6. KNN=5\n",
    "    knn = NearestNeighbors(n_neighbors=5).fit(train_np)\n",
    "    distances, _ = knn.kneighbors(test_np)\n",
    "    knn_scores = distances.mean(axis=1)\n",
    "    results['KNN=5'] = (\n",
    "        roc_auc_score(y_true, knn_scores),\n",
    "        roc_auc_score(y_true, knn_scores, max_fpr=0.1)\n",
    "    )\n",
    "\n",
    "    # === 6. KNN=3\n",
    "    knn = NearestNeighbors(n_neighbors=3).fit(train_np)\n",
    "    distances, _ = knn.kneighbors(test_np)\n",
    "    knn_scores = distances.mean(axis=1)\n",
    "    results['KNN=3'] = (\n",
    "        roc_auc_score(y_true, knn_scores),\n",
    "        roc_auc_score(y_true, knn_scores, max_fpr=0.1)\n",
    "    )\n",
    "\n",
    "    # # === 7. Mahalanobis\n",
    "    # train_mean = np.mean(train_np, axis=0)\n",
    "    # train_cov = np.cov(train_np, rowvar=False)\n",
    "    # try:\n",
    "    #     inv_cov = np.linalg.inv(train_cov)\n",
    "    #     maha_scores = [distance.mahalanobis(x, train_mean, inv_cov) for x in test_np]\n",
    "    #     maha_scores = np.array(maha_scores)\n",
    "    #     results['Mahalanobis'] = (\n",
    "    #         roc_auc_score(y_true, maha_scores),\n",
    "    #         roc_auc_score(y_true, maha_scores, max_fpr=0.1)\n",
    "    #     )\n",
    "    # except np.linalg.LinAlgError:\n",
    "    #     print(\"Mahalanobis: Covariance matrix is not invertible.\")\n",
    "    #     results['Mahalanobis'] = (0.0, 0.0)\n",
    "\n",
    "    # === Print\n",
    "    for method, (auc, pauc) in results.items():\n",
    "        print(f\"{method} → AUC: {auc:.4f}, pAUC: {pauc:.4f}\")\n",
    "\n",
    "    best_method = max(results.items(), key=lambda x: x[1][0])\n",
    "    print(f\"\\nBest Method: {best_method[0]} → AUC: {best_method[1][0]:.4f}, pAUC: {best_method[1][1]:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93cd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score Calculation on Machine: fan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:23<00:00, 78.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy → AUC: 0.9832, pAUC: 0.9534\n",
      "KMeans=60 → AUC: 0.9788, pAUC: 0.9494\n",
      "KMeans=80 → AUC: 0.9788, pAUC: 0.9494\n",
      "GMM=40 → AUC: 0.9740, pAUC: 0.9450\n",
      "GMM=60 → AUC: 0.9744, pAUC: 0.9449\n",
      "LOF=5 → AUC: 0.9845, pAUC: 0.9542\n",
      "LOF=10 → AUC: 0.9840, pAUC: 0.9550\n",
      "KNN=5 → AUC: 0.9778, pAUC: 0.9483\n",
      "KNN=3 → AUC: 0.9780, pAUC: 0.9485\n",
      "\n",
      "Best Method: LOF=5 → AUC: 0.9845, pAUC: 0.9542\n",
      "\n",
      "==================================================\n",
      "\n",
      "Anomaly Score Calculation on Machine: pump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 856/856 [00:10<00:00, 81.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy → AUC: 0.9544, pAUC: 0.8599\n",
      "KMeans=60 → AUC: 0.9427, pAUC: 0.8465\n",
      "KMeans=80 → AUC: 0.9427, pAUC: 0.8465\n",
      "GMM=40 → AUC: 0.9372, pAUC: 0.8493\n",
      "GMM=60 → AUC: 0.9279, pAUC: 0.8381\n",
      "LOF=5 → AUC: 0.9433, pAUC: 0.8454\n",
      "LOF=10 → AUC: 0.9421, pAUC: 0.8453\n",
      "KNN=5 → AUC: 0.9384, pAUC: 0.8472\n",
      "KNN=3 → AUC: 0.9385, pAUC: 0.8476\n",
      "\n",
      "Best Method: CrossEntropy → AUC: 0.9544, pAUC: 0.8599\n",
      "\n",
      "==================================================\n",
      "\n",
      "Anomaly Score Calculation on Machine: slider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1290/1290 [00:16<00:00, 77.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy → AUC: 0.9953, pAUC: 0.9750\n",
      "KMeans=60 → AUC: 0.9963, pAUC: 0.9803\n",
      "KMeans=80 → AUC: 0.9963, pAUC: 0.9803\n",
      "GMM=40 → AUC: 0.9963, pAUC: 0.9804\n",
      "GMM=60 → AUC: 0.9962, pAUC: 0.9799\n",
      "LOF=5 → AUC: 0.9969, pAUC: 0.9836\n",
      "LOF=10 → AUC: 0.9973, pAUC: 0.9858\n",
      "KNN=5 → AUC: 0.9965, pAUC: 0.9818\n",
      "KNN=3 → AUC: 0.9967, pAUC: 0.9828\n",
      "\n",
      "Best Method: LOF=10 → AUC: 0.9973, pAUC: 0.9858\n",
      "\n",
      "==================================================\n",
      "\n",
      "Anomaly Score Calculation on Machine: ToyCar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2459/2459 [00:30<00:00, 79.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy → AUC: 0.9676, pAUC: 0.9011\n",
      "KMeans=60 → AUC: 0.9331, pAUC: 0.8140\n",
      "KMeans=80 → AUC: 0.9331, pAUC: 0.8140\n",
      "GMM=40 → AUC: 0.9112, pAUC: 0.7914\n",
      "GMM=60 → AUC: 0.9030, pAUC: 0.7827\n",
      "LOF=5 → AUC: 0.9341, pAUC: 0.8274\n",
      "LOF=10 → AUC: 0.9369, pAUC: 0.8249\n",
      "KNN=5 → AUC: 0.9203, pAUC: 0.8060\n",
      "KNN=3 → AUC: 0.9196, pAUC: 0.8063\n",
      "\n",
      "Best Method: CrossEntropy → AUC: 0.9676, pAUC: 0.9011\n",
      "\n",
      "==================================================\n",
      "\n",
      "Anomaly Score Calculation on Machine: ToyConveyor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3509/3509 [00:45<00:00, 77.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy → AUC: 0.7790, pAUC: 0.6715\n",
      "KMeans=60 → AUC: 0.7485, pAUC: 0.6436\n",
      "KMeans=80 → AUC: 0.7485, pAUC: 0.6436\n",
      "GMM=40 → AUC: 0.7732, pAUC: 0.6595\n",
      "GMM=60 → AUC: 0.7731, pAUC: 0.6600\n",
      "LOF=5 → AUC: 0.7931, pAUC: 0.6696\n",
      "LOF=10 → AUC: 0.7850, pAUC: 0.6727\n",
      "KNN=5 → AUC: 0.7531, pAUC: 0.6540\n",
      "KNN=3 → AUC: 0.7561, pAUC: 0.6553\n",
      "\n",
      "Best Method: LOF=5 → AUC: 0.7931, pAUC: 0.6696\n",
      "\n",
      "==================================================\n",
      "\n",
      "Anomaly Score Calculation on Machine: valve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 879/879 [00:11<00:00, 77.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy → AUC: 0.9995, pAUC: 0.9974\n",
      "KMeans=60 → AUC: 0.9998, pAUC: 0.9988\n",
      "KMeans=80 → AUC: 0.9998, pAUC: 0.9988\n",
      "GMM=40 → AUC: 0.9992, pAUC: 0.9957\n",
      "GMM=60 → AUC: 0.9992, pAUC: 0.9960\n",
      "LOF=5 → AUC: 0.9983, pAUC: 0.9913\n",
      "LOF=10 → AUC: 0.9986, pAUC: 0.9928\n",
      "KNN=5 → AUC: 0.9996, pAUC: 0.9980\n",
      "KNN=3 → AUC: 0.9996, pAUC: 0.9981\n",
      "\n",
      "Best Method: KMeans=60 → AUC: 0.9998, pAUC: 0.9988\n",
      "\n",
      "==================================================\n",
      "\n",
      "Average of Best Methods:\n",
      "AUC: 0.9465\n",
      "pAUC: 0.8931\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Run\n",
    "# ============================\n",
    "\n",
    "net.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "net.eval()\n",
    "\n",
    "avg_results = {\n",
    "    'CrossEntropy': [0.0, 0.0],\n",
    "    'Cosine': [0.0, 0.0],\n",
    "    'KMeans=40': [0.0, 0.0],\n",
    "    'KMeans=60': [0.0, 0.0],\n",
    "    'GMM=60': [0.0, 0.0],\n",
    "    'GMM=80': [0.0, 0.0],\n",
    "    'LOF=5': [0.0, 0.0],\n",
    "    'LOF=10': [0.0, 0.0],\n",
    "    'KNN=5': [0.0, 0.0],\n",
    "    'KNN=3': [0.0, 0.0],\n",
    "    'Mahalanobis': [0.0, 0.0]\n",
    "}\n",
    "\n",
    "best_method_avg_auc = []\n",
    "best_method_avg_pauc = []\n",
    "\n",
    "for i in range(len(name_list)):\n",
    "    print(f\"Anomaly Score Calculation on Machine: {name_list[i]}\")\n",
    "\n",
    "    test_ds = test_dataset(root_path, name_list[i], name_list)\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    results = anomaly_scores(net=net, arcface=net.arcface, train_loader=train_loader, test_loader=test_loader, device=device)\n",
    "\n",
    "    # Average Results of Each Anomaly Score Methods\n",
    "    # for method, (auc, pauc) in results.items():\n",
    "    #     avg_results[method][0] += auc\n",
    "    #     avg_results[method][1] += pauc\n",
    "\n",
    "    best_method, (best_auc, best_pauc) = max(results.items(), key=lambda x: [1][0])\n",
    "    best_method_avg_auc.append(best_auc)\n",
    "    best_method_avg_pauc.append(best_pauc)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "# Print Average Results of Each Anomaly Score Methods\n",
    "# for method, (total_auc, total_pauc) in avg_results.items():\n",
    "#     avg_auc = total_auc / len(name_list)\n",
    "#     avg_pauc = total_pauc / len(name_list)\n",
    "#     print(f\"Average {method} - AUC: {avg_auc:.4f}, pAUC: {avg_pauc:.4f}\")\n",
    "\n",
    "# Print Average Results of Max Anomaly Score from Each Machine Type\n",
    "print(\"Average of Best Methods:\")\n",
    "print(f\"AUC: {np.mean(best_method_avg_auc):.4f}, pAUC: {np.mean(best_method_avg_pauc):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
